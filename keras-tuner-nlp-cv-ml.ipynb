{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedmustafashaban/keras-tuner-nlp-cv-ml?scriptVersionId=224817405\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Hyperparameter Tuning with Keras Tuner: Optimizing Deep Learning Models**\n\nHyperparameter tuning is a critical step in developing high-performing deep learning models. **Keras Tuner** simplifies this process by automating the search for optimal hyperparameters. In this work, we demonstrate the power of **Keras Tuner** across three different domains: **Computer Vision (MNIST Digit Classification), Regression (California Housing Prices), and Natural Language Processing (20 Newsgroups Text Classification).**\n\n### 1️⃣ Computer Vision: MNIST Digit Classification\nFor the **MNIST handwritten digit classification task**, we build a Convolutional Neural Network (CNN) and tune:\n- Number of convolutional layers\n- Filter sizes\n- Learning rate\n- Batch size\n\nUsing **Hyperband**, we accelerate the search process and identify an optimized model with minimal computational cost.\n\n### 2️⃣ Regression: California Housing Prices\nFor the **California Housing dataset**, we implement a **Fully Connected Neural Network (FCNN)** to predict housing prices based on input features. We optimize:\n- Number of neurons in hidden layers\n- Learning rate\n- Dropout rate\n\nWith **Bayesian Optimization**, we efficiently explore the hyperparameter space and achieve a model that generalizes well on unseen data.\n\n### 3️⃣ NLP: 20 Newsgroups Text Classification\nFor the **20 Newsgroups text classification task**, we use a **Neural Network (NN) for text classification** and optimize key hyperparameters such as:\n- Number of hidden layers\n- Number of neurons per layer\n- Learning rate\n- Batch size\n\nUsing **Random Search**, we identify the best combination of these hyperparameters, leading to improved accuracy on text classification.\n\n### Conclusion\nKeras Tuner provides a powerful and efficient way to optimize hyperparameters across various domains. Whether for **computer vision, regression, or text classification**, leveraging **Random Search, Bayesian Optimization, or Hyperband** enables us to systematically improve model performance. The flexibility and ease of integration make **Keras Tuner** an essential tool for deep learning practitioners.","metadata":{}},{"cell_type":"markdown","source":"1. **Imports**:\n   - `tensorflow`: The main library for building and training neural networks.\n   - `keras`: A high-level API within TensorFlow for building neural networks.\n   - `mnist`: A dataset of handwritten digits.\n   - `Dense`, `Flatten`: Layers used in building the neural network.\n   - `keras_tuner`: A library for hyperparameter tuning.\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.layers import Dense, Flatten\nimport keras_tuner as kt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:48:15.457398Z","iopub.execute_input":"2025-02-27T13:48:15.457688Z","iopub.status.idle":"2025-02-27T13:48:30.800732Z","shell.execute_reply.started":"2025-02-27T13:48:15.457663Z","shell.execute_reply":"2025-02-27T13:48:30.799927Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"2. **Loading Data**:\nThis line loads the MNIST dataset, splitting it into training and testing sets.","metadata":{}},{"cell_type":"code","source":"# Load MNIST Data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:48:30.802252Z","iopub.execute_input":"2025-02-27T13:48:30.802752Z","iopub.status.idle":"2025-02-27T13:48:31.32611Z","shell.execute_reply.started":"2025-02-27T13:48:30.802729Z","shell.execute_reply":"2025-02-27T13:48:31.325367Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"3. **Normalization**:\n   \n   Here, we normalize the pixel values to be between 0 and 1, which helps in faster convergence during training","metadata":{}},{"cell_type":"code","source":"# Normalize the data\nX_train, X_test = X_train / 255.0, X_test / 255.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:48:31.32694Z","iopub.execute_input":"2025-02-27T13:48:31.327268Z","iopub.status.idle":"2025-02-27T13:48:31.502404Z","shell.execute_reply.started":"2025-02-27T13:48:31.327237Z","shell.execute_reply":"2025-02-27T13:48:31.501536Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"4. **Model Building Function**:\n   \n   This function constructs the neural network model. The `hp` parameter allows tuning of hyperparameters.\n\n5. **Adding Layers**:\n\n   This loop adds a tunable number of hidden layers (between 1 and 3). Each layer's size is also tunable.\n\n6. **Output Layer**:\n  \n   The output layer has 10 units (one for each digit) with a softmax activation function to produce probability distributions.\n\n7. **Model Compilation**:\n   \n   The model is compiled with a tunable learning rate and uses the Adam optimizer, which adjusts the learning rate during training.","metadata":{}},{"cell_type":"code","source":"# Function to build a model with tunable parameters\ndef build_model(hp):\n    model = keras.Sequential()\n    model.add(Flatten(input_shape=(28, 28)))\n\n    # Tune the number of hidden layers and units in each layer\n    for i in range(hp.Int('num_layers', 1, 3)):\n        model.add(Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32), activation='relu'))\n\n    # Output layer\n    model.add(Dense(10, activation='softmax'))\n\n    # Compile model with tunable learning rate\n    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:48:31.503148Z","iopub.execute_input":"2025-02-27T13:48:31.50342Z","iopub.status.idle":"2025-02-27T13:48:31.509537Z","shell.execute_reply.started":"2025-02-27T13:48:31.503396Z","shell.execute_reply":"2025-02-27T13:48:31.508485Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"8. **Hyperparameter Tuning**:\n   \n   This sets up the Keras Tuner to randomly search for the best hyperparameters based on validation accuracy.\n\n9. **Executing the Search**:\n  \n   This command runs the hyperparameter tuning process, training the model for 10 epochs while using 20% of the training data for validation.","metadata":{}},{"cell_type":"code","source":"# Use Keras Tuner to find the best hyperparameters\ntuner = kt.RandomSearch(build_model,\n                        objective='val_accuracy',\n                        max_trials=10,  # Number of trials\n                        executions_per_trial=1,\n                        directory='mnist_tuning',\n                        project_name='mnist')\n\n# Execute the search\ntuner.search(X_train, y_train, epochs=10, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:48:31.510519Z","iopub.execute_input":"2025-02-27T13:48:31.510878Z","iopub.status.idle":"2025-02-27T13:53:39.638013Z","shell.execute_reply.started":"2025-02-27T13:48:31.510821Z","shell.execute_reply":"2025-02-27T13:53:39.637201Z"}},"outputs":[{"name":"stdout","text":"Trial 10 Complete [00h 00m 30s]\nval_accuracy: 0.9789166450500488\n\nBest val_accuracy So Far: 0.9789166450500488\nTotal elapsed time: 00h 05m 06s\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"10. **Retrieving Best Hyperparameters**:\n    \n    Finally, this line retrieves the best hyperparameters found during the tuning process and prints them out.","metadata":{}},{"cell_type":"code","source":"# Get the best model hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Best number of layers: {best_hps.get('num_layers')}, Best units per layer: {best_hps.get('units_0')}, Learning rate: {best_hps.get('learning_rate')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:53:39.638902Z","iopub.execute_input":"2025-02-27T13:53:39.639165Z","iopub.status.idle":"2025-02-27T13:53:39.644393Z","shell.execute_reply.started":"2025-02-27T13:53:39.63913Z","shell.execute_reply":"2025-02-27T13:53:39.643305Z"}},"outputs":[{"name":"stdout","text":"Best number of layers: 2, Best units per layer: 128, Learning rate: 0.0005\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Optimization Tips\n- **Increase `max_trials`**: To explore more combinations of hyperparameters.\n- **Adjust `epochs`**: Increase the number of epochs for better training, especially if the model is underfitting.\n- **Early Stopping**: Implement early stopping to prevent overfitting.\n- **Learning Rate Scheduler**: Consider using a learning rate scheduler for better training dynamics.\n\nThis code effectively uses Keras Tuner to optimize a simple neural network for the MNIST dataset by searching through various architectures and hyperparameters.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras_tuner as kt\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:53:39.646893Z","iopub.execute_input":"2025-02-27T13:53:39.647127Z","iopub.status.idle":"2025-02-27T13:53:39.736586Z","shell.execute_reply.started":"2025-02-27T13:53:39.647107Z","shell.execute_reply":"2025-02-27T13:53:39.735775Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load California housing prices dataset\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:53:39.737876Z","iopub.execute_input":"2025-02-27T13:53:39.738217Z","iopub.status.idle":"2025-02-27T13:53:41.099245Z","shell.execute_reply.started":"2025-02-27T13:53:39.738191Z","shell.execute_reply":"2025-02-27T13:53:41.098448Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Normalize the data**\nThe data is normalized to have a mean of 0 and a standard deviation of 1, which is important for training neural networks effectively.","metadata":{}},{"cell_type":"code","source":"# Normalize the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:53:41.100103Z","iopub.execute_input":"2025-02-27T13:53:41.10042Z","iopub.status.idle":"2025-02-27T13:53:41.113588Z","shell.execute_reply.started":"2025-02-27T13:53:41.100386Z","shell.execute_reply":"2025-02-27T13:53:41.112813Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Function to build the model\ndef build_model(hp):\n    model = tf.keras.Sequential()\n    \n    # Tune the number of hidden layers and the number of units in each layer\n    for i in range(hp.Int('num_layers', 1, 3)):\n        model.add(tf.keras.layers.Dense(units=hp.Int(f'units_{i}', 16, 128, step=16), activation='relu'))\n    \n    model.add(tf.keras.layers.Dense(1))  # Output layer\n\n    # Tune the learning rate\n    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n                  loss='mse',\n                  metrics=['mae'])\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:53:41.114693Z","iopub.execute_input":"2025-02-27T13:53:41.115174Z","iopub.status.idle":"2025-02-27T13:53:41.120484Z","shell.execute_reply.started":"2025-02-27T13:53:41.115149Z","shell.execute_reply":"2025-02-27T13:53:41.119482Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create Keras Tuner for finding the best model\ntuner = kt.BayesianOptimization(build_model,\n                                objective='val_mae',\n                                max_trials=10,\n                                directory='housing_tuning',\n                                project_name='housing')\n\n# Execute the search\ntuner.search(X_train, y_train, epochs=10, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:53:41.121538Z","iopub.execute_input":"2025-02-27T13:53:41.12189Z","iopub.status.idle":"2025-02-27T13:55:32.673707Z","shell.execute_reply.started":"2025-02-27T13:53:41.121855Z","shell.execute_reply":"2025-02-27T13:55:32.672979Z"}},"outputs":[{"name":"stdout","text":"Trial 10 Complete [00h 00m 13s]\nval_mae: 0.4646338224411011\n\nBest val_mae So Far: 0.38897740840911865\nTotal elapsed time: 00h 01m 52s\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Display the best hyperparameters found\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Best number of layers: {best_hps.get('num_layers')}, Best units: {best_hps.get('units_0')}, Learning rate: {best_hps.get('learning_rate')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:32.674531Z","iopub.execute_input":"2025-02-27T13:55:32.674781Z","iopub.status.idle":"2025-02-27T13:55:32.679382Z","shell.execute_reply.started":"2025-02-27T13:55:32.674758Z","shell.execute_reply":"2025-02-27T13:55:32.67855Z"}},"outputs":[{"name":"stdout","text":"Best number of layers: 3, Best units: 112, Learning rate: 0.0005\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"\n1. **Imports**:\n   - `tensorflow`: For building and training the neural network.\n   - `keras_tuner`: For hyperparameter tuning.\n   - Layers such as `Embedding`, `LSTM`, `Dense`, and `Bidirectional` are imported for constructing the model.\n   - `Tokenizer` and `pad_sequences` are used for preprocessing text data.\n   - `train_test_split`: For splitting the dataset into training and testing sets.\n   - `fetch_20newsgroups`: To load the dataset for text classification.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras_tuner as kt\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_20newsgroups","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:32.680296Z","iopub.execute_input":"2025-02-27T13:55:32.680594Z","iopub.status.idle":"2025-02-27T13:55:32.693419Z","shell.execute_reply.started":"2025-02-27T13:55:32.68056Z","shell.execute_reply":"2025-02-27T13:55:32.692617Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"\n2. **Loading Data**:\n\n   This code fetches the 20 Newsgroups dataset and splits it into training and testing sets, using 20% of the data for testing.","metadata":{}},{"cell_type":"code","source":"# Load news data\ndata = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.space'], remove=('headers', 'footers', 'quotes'))\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:32.694281Z","iopub.execute_input":"2025-02-27T13:55:32.694489Z","iopub.status.idle":"2025-02-27T13:55:44.245282Z","shell.execute_reply.started":"2025-02-27T13:55:32.694469Z","shell.execute_reply":"2025-02-27T13:55:44.244427Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"3. **Text Preprocessing**:\n\n   The `Tokenizer` converts text data into sequences of integers, and `pad_sequences` ensures that all sequences have the same length (200 in this case).","metadata":{}},{"cell_type":"code","source":"# Convert texts to numerical sequences\ntokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\nX_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=200, padding='post', truncating='post')\nX_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=200, padding='post', truncating='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:44.24615Z","iopub.execute_input":"2025-02-27T13:55:44.246413Z","iopub.status.idle":"2025-02-27T13:55:44.584908Z","shell.execute_reply.started":"2025-02-27T13:55:44.246393Z","shell.execute_reply":"2025-02-27T13:55:44.584143Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"4. **Building the Model**:\n   \n   This function defines the model architecture. The `hp` parameter allows for tuning of hyperparameters.\n\n5. **Adding Layers**:\n  \n   This loop adds a tunable number of Bidirectional LSTM layers (from 1 to 3) with tunable units (from 32 to 128, in steps of 32).\n\n6. **Final Layer**:\n   \n   The final LSTM layer has a fixed size of 64 units, and the output layer uses a sigmoid activation function for binary classification.\n\n7. **Compiling the Model**:\n \n   The model is compiled with the Adam optimizer, binary cross-entropy loss, and accuracy as a metric. The learning rate is also tunable.","metadata":{}},{"cell_type":"code","source":"# Function to build the model\ndef build_model(hp):\n    model = tf.keras.Sequential()\n    model.add(Embedding(input_dim=10000, output_dim=hp.Int('embedding_dim', 32, 128, step=32), input_length=200))\n    \n    # Tune the number of LSTM layers\n    for i in range(hp.Int('num_layers', 1, 3)):\n        model.add(Bidirectional(LSTM(units=hp.Int(f'lstm_units_{i}', 32, 128, step=32), return_sequences=True)))\n    \n    model.add(Bidirectional(LSTM(64)))  # Final layer\n    model.add(Dense(1, activation='sigmoid'))  # Output layer\n\n    # Tune the learning rate\n    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:44.585711Z","iopub.execute_input":"2025-02-27T13:55:44.585969Z","iopub.status.idle":"2025-02-27T13:55:44.592089Z","shell.execute_reply.started":"2025-02-27T13:55:44.585948Z","shell.execute_reply":"2025-02-27T13:55:44.591064Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"8. **Setting Up Keras Tuner**:\n\n   This initializes the Keras Tuner using Hyperband optimization to find the best hyperparameters based on validation accuracy.","metadata":{}},{"cell_type":"code","source":"# Search for the best hyperparameters using Keras Tuner\ntuner = kt.Hyperband(build_model,\n                     objective='val_accuracy',\n                     max_epochs=10,\n                     directory='text_tuning',\n                     project_name='text_classification')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:44.592966Z","iopub.execute_input":"2025-02-27T13:55:44.593227Z","iopub.status.idle":"2025-02-27T13:55:44.64565Z","shell.execute_reply.started":"2025-02-27T13:55:44.593205Z","shell.execute_reply":"2025-02-27T13:55:44.644776Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"9. **Executing the Search**:\n\n   This command runs the hyperparameter tuning process over 10 epochs, using 20% of the training data for validation.","metadata":{}},{"cell_type":"code","source":"# Execute the search\ntuner.search(X_train_seq, y_train, epochs=10, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:55:44.646576Z","iopub.execute_input":"2025-02-27T13:55:44.646961Z","iopub.status.idle":"2025-02-27T14:02:21.502748Z","shell.execute_reply.started":"2025-02-27T13:55:44.646924Z","shell.execute_reply":"2025-02-27T14:02:21.502009Z"}},"outputs":[{"name":"stdout","text":"Trial 30 Complete [00h 00m 32s]\nval_accuracy: 0.9085173606872559\n\nBest val_accuracy So Far: 0.9274448156356812\nTotal elapsed time: 00h 06m 37s\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"10. **Retrieving Best Hyperparameters**:\n  \n    This line retrieves the best hyperparameters found during the tuning process and prints them.","metadata":{}},{"cell_type":"code","source":"# Display the best hyperparameters found\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"Best number of layers: {best_hps.get('num_layers')}, Best LSTM units: {best_hps.get('lstm_units_0')}, Learning rate: {best_hps.get('learning_rate')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:02:21.503613Z","iopub.execute_input":"2025-02-27T14:02:21.503894Z","iopub.status.idle":"2025-02-27T14:02:21.508123Z","shell.execute_reply.started":"2025-02-27T14:02:21.503858Z","shell.execute_reply":"2025-02-27T14:02:21.507182Z"}},"outputs":[{"name":"stdout","text":"Best number of layers: 1, Best LSTM units: 64, Learning rate: 0.0005\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Optimization Tips\n- **Adjust `num_words` in Tokenizer**: Increasing this can capture more vocabulary, but may also increase complexity.\n- **Early Stopping**: Implement early stopping to prevent overfitting if the validation accuracy stops improving.\n- **Experiment with Dropout Layers**: Adding dropout layers can help reduce overfitting.\n\nThis code effectively tunes a Bidirectional LSTM model for classifying text data, showcasing the capabilities of Keras Tuner for optimizing hyperparameters in deep learning models","metadata":{}}]}